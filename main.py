import os
import pandas as pd
from openai import OpenAI
from sentence_transformers import SentenceTransformer

from context_utils.context_retriever import ContextRetriever


# ë³‘ë ¬ ì²˜ë¦¬ ë¹„í™œì„±í™”
os.environ["TOKENIZERS_PARALLELISM"] = "false"

def load_data(file_path):
    """ì—‘ì…€ íŒŒì¼ì„ ì½ì–´ DataFrameìœ¼ë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
    data = pd.read_excel(file_path)
    print("ë°ì´í„° ë¡œë“œ ì™„ë£Œ!")
    print(data.head())
    return data

def process_with_openai(data, base_url, context_retriever):
    """OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•´ ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
    results = []

    for index, row in data.iterrows():

        print(f"\nğŸ”„ ì²˜ë¦¬ ì¤‘ì¸ ì§ˆë¬¸ (ID: {row['id']}): {row['question']}")

        # ê¸°ì¡´ add_feature í•¨ìˆ˜ ì‚¬ìš©í•˜ì—¬ ë©”ì‹œì§€ êµ¬ì¡° ìƒì„±
        message_structure = add_feature(row["id"], row["question"], context_retriever)

        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        client = OpenAI(
            base_url=base_url,
            api_key="dummy-key",
            default_headers={
                "Content-Type": "application/json",
                "Question-ID": str(row["id"])
            }
        )

        try:
            response = client.chat.completions.create(
                model="olympiad",
                messages=message_structure["message"],
                temperature=0.0,
                max_tokens=None,
                stream=False
            )

            # responseë¥¼ dictionaryë¡œ ë³€í™˜
            response_dict = response.model_dump()
            result = response_dict.get('result', {})

            print(f"\nâœ… ID {row['id']} ì²˜ë¦¬ ì™„ë£Œ")
            print(f"ì‘ë‹µ: {result.get('response', '')}")

            results.append({
                'id': row['id'],
                'question': row['question'],
                'prompt': result.get('prompt', ''),
                'context': result.get('context', ''),
                'response': result.get('response', ''),
                'score': result.get('score', 0),
                'reasoning': result.get('reasoning', '')
            })

        except Exception as e:
            print(f"ID {row['id']} ì²˜ë¦¬ ì¤‘ ì—ëŸ¬ ë°œìƒ: {str(e)}")


    # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜í•˜ê³  ì—‘ì…€ë¡œ ì €ì¥
    results_df = pd.DataFrame(results)
    results_df.to_excel('response_results.xlsx', index=False)

def add_feature(id, question, context_retriever):
    """
    ë©”ì‹œì§€ êµ¬ì¡°ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    - ì´ í•¨ìˆ˜ëŠ” í•™ìƒë“¤ì´ í•„ìš”ì— ë”°ë¼ ì‹œìŠ¤í…œ ë©”ì‹œì§€ë‚˜ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì¶”ê°€ë¡œ ì •ì˜í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.
    - ì•„ë˜ì˜ system_promptì™€ user_messageë¥¼ ìˆ˜ì •í•˜ì—¬ ì»¤ìŠ¤í„°ë§ˆì´ì¦ˆí•˜ì„¸ìš”.

    :param id: int/str, ID
    :param question: str, ì§ˆë¬¸
    :return: dict, ë©”ì‹œì§€ JSON êµ¬ì¡°
    """
    # -----------------ìˆ˜ì •--------------------#
    system_prompt = """
Persona:
You are an expert in artificial intelligence, machine learning, data analysis, and IT trends.

Instructions:
	1.	Base your response strictly on the provided question and sources. Keep it accurate and concise.
	2.	Do not include information outside the provided sources.
	3.	Exclude topics or details not explicitly mentioned in the sources.
	4.	If a specific format is requested, adhere to it strictly.
	5.	Include examples or cases only if explicitly mentioned in the sources.

Limit:
    1. Responses must not exceed 1,500 characters.
    2. Your response must be written in Korean.
    """

    # -----------------ìˆ˜ì •--------------------#
    # ì‚¬ìš©ì ë©”ì‹œì§€ ìƒì„± (add_rag í•¨ìˆ˜ì—ì„œ ì¶”ê°€ ì²˜ë¦¬)
    user_message = add_rag(question, context_retriever)

    # ë©”ì‹œì§€ êµ¬ì¡° ë°˜í™˜
    message = {
        "id": id,
        "message": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]
    }
    return message


def add_rag(question, context_retriever):
    """
    ì§ˆë¬¸ì— ì¶”ê°€ ì •ë³´ë¥¼ ê²°í•©í•˜ëŠ” í•¨ìˆ˜
    - ì´ í•¨ìˆ˜ëŠ” í•™ìƒë“¤ì´ RAG(Retrieval-Augmented Generation)ë¥¼ êµ¬í˜„í•˜ê±°ë‚˜, ì§ˆë¬¸ì— ì¶”ê°€ ì •ë³´ë¥¼ ì‚½ì…í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.
    - contextë¥¼ í™œìš©í•´ ì§ˆë¬¸ì— ì •ë³´ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.

    :param question: str, ì§ˆë¬¸
    :return: str, ìˆ˜ì •ëœ ì§ˆë¬¸
    """
    # -----------------ìˆ˜ì •--------------------#
    """
    ìˆ˜ì •ì‚¬í•­
    - ì™¸ë¶€ì—ì„œ ì „ë‹¬ë°›ì€ ContextRetrieverë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. (ëª¨ë¸ì„ í•œë²ˆë§Œ ë¡œë“œí•˜ê¸° ìœ„í•¨)
    - FAISSë¥¼ ì‚¬ìš©í•´ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê²€ìƒ‰í•˜ê³  ê²°í•©í•˜ë„ë¡ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤.
        - ì§ˆë¬¸ì˜ ë²¡í„°ê°’ê³¼ ì»¨í…ìŠ¤íŠ¸ (ë°°ê²½ì§€ì‹)ì´ ì—°ê²°ë˜ì–´ìˆìŒ.
        - ì§ˆë¬¸ì´ ë“¤ì–´ì˜¤ë©´, í•´ë‹¹ ì§ˆë¬¸ì˜ ë²¡í„°ê°’ê³¼ ì¸ë±ì‹±ë˜ì–´ìˆëŠ” ì§ˆë¬¸ì˜ ë²¡í„°ê°’ê³¼ ë¹„êµ í›„ í•´ë‹¹ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¦¬í„´
    """
    # ContextRetriever ì´ˆê¸°í™” (ì„ë² ë”© íŒŒì¼ ê²½ë¡œ ì§€ì •)
    context = context_retriever.get_related_contexts(question)

    # -----------------ìˆ˜ì •--------------------#
    # ì§ˆë¬¸ì— ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ (ì˜ˆ: "<BEGIN SOURCE>" í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ê²°í•©)
    if context:
        # UIì™€ ê°™ì€ í˜•ì‹ ìœ ì§€ë¥¼ ìœ„í•œ ë³€ê²½ ë¶ˆê°€
        question = question + '\n\nYou may use the following sources if needed to answer the user\'s question. If you don\'t know the answer, say "I don\'t know."\n\n<BEGIN SOURCE>' + context
    return question


# ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„
if __name__ == "__main__":
    file_path = './data/problem.xlsx'
    embedding_file = './data/embeddings.pkl'
    base_url = "https://ryeon.elpai.org/submit/v1"

    # ë°ì´í„° ë¡œë“œ
    data = load_data(file_path)

    # ëª¨ë¸ ë¡œë“œ
    print("\nğŸ”„ SentenceTransformer ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model = SentenceTransformer('all-MiniLM-L6-v2')
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

    # ContextRetriever ì´ˆê¸°í™”
    context_retriever = ContextRetriever(embedding_file, model)

    process_with_openai(data, base_url, context_retriever)